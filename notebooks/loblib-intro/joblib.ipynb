{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da182c5a",
   "metadata": {},
   "source": [
    "## Using joblib to speed up your Python pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5a7518",
   "metadata": {},
   "source": [
    "### Why joblib?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92105fbf",
   "metadata": {},
   "source": [
    "There are several reasons to integrate joblib tools as a part of the ML pipeline. There are major two reasons mentioned on their [website](https://joblib.readthedocs.io/en/latest/) to use it. However, I thought to rephrase it again:\n",
    "- Capability to use cache which avoids recomputation of some of the steps,\n",
    "- Execute Parallelization to fully utilize all the cores of CPU/GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188f7896",
   "metadata": {},
   "source": [
    "Beyond this, there are several other reasons why I would recommend joblib:\n",
    "1. Can be easily integrated\n",
    "2. No specific dependencies\n",
    "3. Saves cost and time\n",
    "4. Easy to learn\n",
    "\n",
    "There are other functionalities that are also resourceful and help greatly if included in daily work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7131cda",
   "metadata": {},
   "source": [
    "### 1. Using Cached results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d769f9d2",
   "metadata": {},
   "source": [
    "It often happens, that we need to re-run our pipelines multiple times while testing or creating the model. Some of the functions might be called several times, with the same input data and the computation happens again. Joblib provides a better way to avoid recomputing the same function repetitively saving a lot of time and computational cost. For example, let's take a simple example below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ce68f84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The function took 20.18 s to compute.\n",
      "[1, 4, 9, 16, 25, 36, 49, 64, 81, 100, 121, 144, 169, 196, 225, 256, 289, 324, 361, 400]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "result = []\n",
    "\n",
    "# Getting the square of the number:\n",
    "def square_number(no):\n",
    "    return (no*no)\n",
    "\n",
    "# Function to compute square of a range of a number:\n",
    "def get_square_range(start_no, end_no):\n",
    "    for i in np.arange(start_no, end_no):\n",
    "        time.sleep(1)\n",
    "        result.append(square_number(i))\n",
    "    return result\n",
    "\n",
    "start = time.time()\n",
    "# Getting square of 1 to 20:\n",
    "final_result = get_square_range(1, 21)\n",
    "end = time.time()\n",
    "\n",
    "# Total time to compute\n",
    "print('\\nThe function took {:.2f} s to compute.'.format(end - start))\n",
    "print(final_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90af44e4",
   "metadata": {},
   "source": [
    "As seen above, the function is simply computing the square of a number over a range provided. It takes ~20 s to get the result. Now, let's use joblib’s Memory function with a location defined to store a cache as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e4a7cefe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The function took 20.17 s to compute.\n",
      "[1, 4, 9, 16, 25, 36, 49, 64, 81, 100, 121, 144, 169, 196, 225, 256, 289, 324, 361, 400]\n"
     ]
    }
   ],
   "source": [
    "from joblib import Memory\n",
    "\n",
    "# Define a location to store cache\n",
    "location = 'cache_dir'\n",
    "memory = Memory(location, verbose=0)\n",
    "\n",
    "result = []\n",
    "\n",
    "# Function to compute square of a range of a number:\n",
    "def get_square_range_cached(start_no, end_no):\n",
    "    for i in np.arange(start_no, end_no):\n",
    "        time.sleep(1)\n",
    "        result.append(square_number(i))\n",
    "    return result\n",
    "\n",
    "get_square_range_cached = memory.cache(get_square_range_cached)\n",
    "\n",
    "start = time.time()\n",
    "# Getting square of 1 to 50:\n",
    "final_result = get_square_range_cached(1, 21)\n",
    "end = time.time()\n",
    "\n",
    "# Total time to compute\n",
    "print('\\nThe function took {:.2f} s to compute.'.format(end - start))\n",
    "print(final_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6961aad7",
   "metadata": {},
   "source": [
    "On computing the first time, the result is pretty much the same as before of ~20 s, because the results are computing the first time and then getting stored to a location. Let's try running one more time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3b8c419b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The function took 0.00 s to compute.\n",
      "[1, 4, 9, 16, 25, 36, 49, 64, 81, 100, 121, 144, 169, 196, 225, 256, 289, 324, 361, 400]\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "# Getting square of 1 to 50:\n",
    "final_result = get_square_range_cached(1, 21)\n",
    "end = time.time()\n",
    "\n",
    "print('\\nThe function took {:.2f} s to compute.'.format(end - start))\n",
    "print(final_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44659e1b",
   "metadata": {},
   "source": [
    "And VOILA! It took 0.01 s to provide the results. The time reduced almost by 2000x. This is mainly because the results were already computed and stored in a cache on the computer. The efficiency rate will not be the same for all the functions! It might vary majorly for the type of computation requested. But you will definitely have this superpower to expedite the pipeline by caching!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07dd56d7",
   "metadata": {},
   "source": [
    "To clear the cache results, it is possible using a direct command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7211d667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean-up the cache folder\n",
    "memory.clear(warn=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954fa4eb",
   "metadata": {},
   "source": [
    "Be careful though, before using this code. You might wipe out your work worth weeks of computation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3bd25ea",
   "metadata": {},
   "source": [
    "### 2. Parallelization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a513ed17",
   "metadata": {},
   "source": [
    "As the name suggests, we can compute in parallel any specified function with even multiple arguments using “joblib.Parallel”. Behind the scenes, when using multiple jobs (if specified), each calculation does not wait for the previous one to complete and can use different processors to get the task done. For better understanding, I have shown how Parallel jobs can be run inside caching."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11202ad1",
   "metadata": {},
   "source": [
    "Consider the following random dataset generated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d4f027e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(42)\n",
    "data = rng.randn(int(1e4), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f75bc1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 4)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e348a065",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.49671415, -0.1382643 ,  0.64768854,  1.52302986],\n",
       "       [-0.23415337, -0.23413696,  1.57921282,  0.76743473],\n",
       "       [-0.46947439,  0.54256004, -0.46341769, -0.46572975],\n",
       "       [ 0.24196227, -1.91328024, -1.72491783, -0.56228753],\n",
       "       [-1.01283112,  0.31424733, -0.90802408, -1.4123037 ],\n",
       "       [ 1.46564877, -0.2257763 ,  0.0675282 , -1.42474819],\n",
       "       [-0.54438272,  0.11092259, -1.15099358,  0.37569802],\n",
       "       [-0.60063869, -0.29169375, -0.60170661,  1.85227818],\n",
       "       [-0.01349722, -1.05771093,  0.82254491, -1.22084365],\n",
       "       [ 0.2088636 , -1.95967012, -1.32818605,  0.19686124]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a023b3a",
   "metadata": {},
   "source": [
    "Below is a run with our normal sequential processing, where a new calculation starts only after the previous calculation is completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e5abd766",
   "metadata": {},
   "outputs": [],
   "source": [
    "def costly_compute(data, column):\n",
    "    \"\"\"Emulate a costly function by sleeping and returning a column.\"\"\"\n",
    "    time.sleep(2)\n",
    "    return data[column]\n",
    "\n",
    "def data_processing_mean(data, column):\n",
    "    \"\"\"Compute the mean of a column.\"\"\"\n",
    "    return costly_compute(data, column).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "76c894da",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "results = [data_processing_mean(data, col) for col in range(data.shape[1])]\n",
    "stop = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ff8c444b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sequential processing\n",
      "Elapsed time for the entire processing: 8.04 s\n"
     ]
    }
   ],
   "source": [
    "print('\\nSequential processing')\n",
    "print('Elapsed time for the entire processing: {:.2f} s'\n",
    "      .format(stop - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f12111",
   "metadata": {},
   "source": [
    "For parallel processing, we set the number of jobs = 2. The number of jobs is limit to the number of cores the CPU has or are available (idle)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "47f49498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time for the entire processing: 4.37 s\n"
     ]
    }
   ],
   "source": [
    "#Import package\n",
    "from joblib import Parallel, delayed\n",
    "from joblib import Memory\n",
    "\n",
    "location = 'cache_dir'\n",
    "memory = Memory(location, verbose=0)\n",
    "costly_compute_cached = memory.cache(costly_compute)\n",
    "\n",
    "def data_processing_mean_using_cache(data, column):\n",
    "    \"\"\"Compute the mean of a column.\"\"\"\n",
    "    return costly_compute_cached(data, column).mean()\n",
    "\n",
    "start = time.time()\n",
    "results = Parallel(n_jobs=2)(\n",
    "    delayed(data_processing_mean_using_cache)(data, col)\n",
    "    for col in range(data.shape[1]))\n",
    "stop = time.time()\n",
    "\n",
    "print('Elapsed time for the entire processing: {:.2f} s'\n",
    "      .format(stop - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa77562",
   "metadata": {},
   "source": [
    "Here we can see that time for processing using the Parallel method was reduced by **2x**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4f0640",
   "metadata": {},
   "source": [
    "- **Note**: using this method may show deteriorated performance if used for less computational intensive functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395ec0cf",
   "metadata": {},
   "source": [
    "### 3. Dump and Load"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f800b3dd",
   "metadata": {},
   "source": [
    "We often need to store and load the datasets, models, computed results, etc. to and from a location on the computer. Joblib provides functions that can be used to dump and load easily:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5a1ede00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dump duration: 0.002s\n"
     ]
    }
   ],
   "source": [
    "from joblib import dump, load\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "path = 'dumped/'\n",
    "# Name of the file\n",
    "joblib_file = 'train_features.joblib'\n",
    "\n",
    "dump(data, path + joblib_file)\n",
    "\n",
    "# Calculating the total time\n",
    "simple_joblib_duration = time.time() - start\n",
    "print(\"Dump duration: %0.3fs\" % simple_joblib_duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "0be128fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load duration: 0.001s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "data = load(path + joblib_file)\n",
    "\n",
    "# Calculating the total time\n",
    "simple_joblib_duration = time.time() - start\n",
    "print(\"Load duration: %0.3fs\" % simple_joblib_duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a9da756f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.49671415, -0.1382643 ,  0.64768854,  1.52302986],\n",
       "       [-0.23415337, -0.23413696,  1.57921282,  0.76743473],\n",
       "       [-0.46947439,  0.54256004, -0.46341769, -0.46572975],\n",
       "       [ 0.24196227, -1.91328024, -1.72491783, -0.56228753],\n",
       "       [-1.01283112,  0.31424733, -0.90802408, -1.4123037 ],\n",
       "       [ 1.46564877, -0.2257763 ,  0.0675282 , -1.42474819],\n",
       "       [-0.54438272,  0.11092259, -1.15099358,  0.37569802],\n",
       "       [-0.60063869, -0.29169375, -0.60170661,  1.85227818],\n",
       "       [-0.01349722, -1.05771093,  0.82254491, -1.22084365],\n",
       "       [ 0.2088636 , -1.95967012, -1.32818605,  0.19686124]])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848bbd95",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033f36dd",
   "metadata": {},
   "source": [
    "### 4. Compression methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397f3ef4",
   "metadata": {},
   "source": [
    "When dealing with larger datasets the size occupied by these files is massive. With feature engineering, the file size gets even larger as we add more columns. Fortunately, nowadays, with the storages getting so cheap, it is less of an issue. However, still, to be efficient there are some compression methods that joblib provides are very simple to use:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10886e6",
   "metadata": {},
   "source": [
    "**a. Simple Compression:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "257ddded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw dump duration: 0.002s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# File\n",
    "joblib_file = '/train_features.joblib'\n",
    "\n",
    "# Dumping the file in the normal format\n",
    "dump(data, path + joblib_file)\n",
    "    \n",
    "simple_pickle_duration = time.time() - start\n",
    "\n",
    "# total time taken to dump\n",
    "print(\"Raw dump duration: %0.3fs\" % simple_pickle_duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "09d2cfe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file size is: 0.320MB\n"
     ]
    }
   ],
   "source": [
    "# joblib_file = '/train_features.joblib'\n",
    "\n",
    "# Measuring the size of the file\n",
    "raw_file_size = os.stat(path + joblib_file).st_size / 1e6\n",
    "\n",
    "# Printing the size\n",
    "print(\"The file size is: %0.3fMB\" % raw_file_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb13f92c",
   "metadata": {},
   "source": [
    "**b. Using Zlib compression:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "39f9f623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zlib dump duration: 0.012s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# File\n",
    "joblib_file_zlib = 'joblib_file_zlib.joblib'\n",
    "\n",
    "# Dumping the file in the zlib compression format\n",
    "dump(data, path + joblib_file_zlib, compress='zlib')\n",
    "\n",
    "zlib_joblib_duration = time.time() - start\n",
    "\n",
    "# Total time taken to dump\n",
    "print(\"Zlib dump duration: %0.3fs\" % zlib_joblib_duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "42f5fe00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file size is: 0.309MB\n"
     ]
    }
   ],
   "source": [
    "# Measuring the size of the file\n",
    "zlib_file_size = os.stat(path + joblib_file_zlib).st_size / 1e6\n",
    "\n",
    "# Printing the size\n",
    "print(\"The file size is: %0.3fMB\" % zlib_file_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68866f31",
   "metadata": {},
   "source": [
    "**There is another comperssion algorithm - lz4.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02f30f0",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
